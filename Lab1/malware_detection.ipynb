{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção de Malware em Aplicações Android - Mineração de Dados em Larga Escala\n",
    "\n",
    "### Introdução  \n",
    "Com o crescimento da utilização de dispositivos móveis, a **segurança digital** tornou-se uma preocupação essencial, especialmente para o sistema **Android**. A presença de aplicações maliciosas (**malware**) pode comprometer a privacidade dos utilizadores, roubar dados sensíveis e causar prejuízos financeiros.  \n",
    "\n",
    "Este trabalho tem como objetivo **reproduzir e validar os resultados do artigo científico** que analisou a deteção de malware em aplicações Android utilizando **algoritmos de Machine Learning**. Para isso, utilizamos o **dataset TUANDROMD**, composto por amostras de aplicações **benignas (goodware)** e **maliciosas (malware)**, com base nas permissões e chamadas de API utilizadas.  \n",
    "\n",
    "### Objetivo do Trabalho  \n",
    "O objetivo principal é **testar e comparar diferentes algoritmos de Machine Learning** para avaliar **quais modelos são mais eficazes** na deteção de malware. Foram explorados **diferentes combinações de hiperparâmetros** para otimizar o desempenho dos modelos, bem como a reprodução de toda a informação disponível no artigo.\n",
    "\n",
    "Além disso, este trabalho pretende validar os resultados apresentados no artigo original.  \n",
    "\n",
    "### Dataset Utilizado  \n",
    "O dataset **TUANDROMD** contém **permissões e chamadas de API´s** extraídas de aplicações Android, que foram classificadas da seguinte forma:  \n",
    "- `0` → **Goodware**  \n",
    "- `1` → **Malware** \n",
    "\n",
    "Os dados foram pré-processados e divididos de forma a garantir **equilíbrio entre classes** e **evitar overfitting**.  \n",
    "\n",
    "### Modelos Testados  \n",
    "Foram testados cinco algoritmos de Machine Learning, cada um com diferentes configurações de **número de estimadores**, **profundidade das árvores** e **taxa de aprendizagem**:  \n",
    "\n",
    "- **Random Forest**   \n",
    "- **Extra Trees**   \n",
    "- **AdaBoost**   \n",
    "- **XGBoost**   \n",
    "- **Gradient Boosting**   \n",
    "\n",
    "Cada modelo foi treinado com a utilização de **validação cruzada (10-Fold Cross-Validation)**, como foi referido no artigo, e avaliado através de **métricas de desempenho**, como **Accuracy, Precision, Recall e F1-Score**.  \n",
    "\n",
    "### O que foi feito no código abaixo  \n",
    "O código implementa os seguintes passos:  \n",
    "1. **Tratamento de dados**: Leitura do dataset e separação das features e rótulos.  \n",
    "2. **Pré-processamento**: Normalização e \"balanceamento\" dos dados.  \n",
    "3. **Treino dos modelos**: Cada algoritmo foi testado com **diferentes combinações de hiperparâmetros**.  \n",
    "4. **Avaliação dos resultados**: Utilização de **validação cruzada** e cálculo de métricas como **Accuracy, Precision, Recall e F1-Score**.  \n",
    "5. **Comparação dos modelos**: Identificação do modelo que obteve **melhores resultados na deteção de malware**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise do Dataset  \n",
    "\n",
    "Antes de treinar os modelos, foi realizada uma breve análise de dados para compreender melhor a estrutura do dataset e a distribuição das classes.  \n",
    "\n",
    "### O que foi feito  \n",
    "- **Visualização das primeiras linhas** do dataset (`head()`) para verificar a estrutura geral.  \n",
    "- **Análise das informações do dataset** (`info()`) para identificar o número de amostras, tipos de dados e valores em falta.  \n",
    "- **Avaliação da distribuição das classes** para verificar possíveis desequilíbrios entre malware e goodware.  \n",
    "- **Visualização da distribuição das classes** através de um gráfico de barras para observar a proporção entre malware e goodware.  \n",
    "\n",
    "### Observações  \n",
    "- O dataset contém **duas classes**:  \n",
    "  - **0** → Goodware\n",
    "  - **1** → Malware\n",
    "- Caso exista um desequilíbrio significativo entre as classes (como será visto no gráfico), pode ser necessário aplicar técnicas de **\"balanceamento\"** de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ACCESS_ALL_DOWNLOADS  ACCESS_CACHE_FILESYSTEM  ACCESS_CHECKIN_PROPERTIES  \\\n",
      "0                   0.0                      0.0                        0.0   \n",
      "1                   0.0                      0.0                        0.0   \n",
      "2                   0.0                      0.0                        0.0   \n",
      "3                   0.0                      0.0                        0.0   \n",
      "4                   0.0                      0.0                        0.0   \n",
      "\n",
      "   ACCESS_COARSE_LOCATION  ACCESS_COARSE_UPDATES  ACCESS_FINE_LOCATION  \\\n",
      "0                     0.0                    0.0                   0.0   \n",
      "1                     0.0                    0.0                   0.0   \n",
      "2                     0.0                    0.0                   0.0   \n",
      "3                     0.0                    0.0                   0.0   \n",
      "4                     0.0                    0.0                   0.0   \n",
      "\n",
      "   ACCESS_LOCATION_EXTRA_COMMANDS  ACCESS_MOCK_LOCATION  ACCESS_MTK_MMHW  \\\n",
      "0                             0.0                   0.0              0.0   \n",
      "1                             0.0                   0.0              0.0   \n",
      "2                             0.0                   0.0              0.0   \n",
      "3                             0.0                   0.0              0.0   \n",
      "4                             0.0                   0.0              0.0   \n",
      "\n",
      "   ACCESS_NETWORK_STATE  ...  \\\n",
      "0                   1.0  ...   \n",
      "1                   1.0  ...   \n",
      "2                   1.0  ...   \n",
      "3                   0.0  ...   \n",
      "4                   0.0  ...   \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getLine1Number  \\\n",
      "0                                                1.0      \n",
      "1                                                0.0      \n",
      "2                                                0.0      \n",
      "3                                                0.0      \n",
      "4                                                0.0      \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getNetworkOperator  \\\n",
      "0                                                1.0          \n",
      "1                                                0.0          \n",
      "2                                                0.0          \n",
      "3                                                1.0          \n",
      "4                                                0.0          \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getNetworkOperatorName  \\\n",
      "0                                                1.0              \n",
      "1                                                0.0              \n",
      "2                                                0.0              \n",
      "3                                                1.0              \n",
      "4                                                0.0              \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getNetworkCountryIso  \\\n",
      "0                                                0.0            \n",
      "1                                                1.0            \n",
      "2                                                0.0            \n",
      "3                                                1.0            \n",
      "4                                                0.0            \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getSimOperator  \\\n",
      "0                                                0.0      \n",
      "1                                                0.0      \n",
      "2                                                0.0      \n",
      "3                                                1.0      \n",
      "4                                                0.0      \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getSimOperatorName  \\\n",
      "0                                                0.0          \n",
      "1                                                0.0          \n",
      "2                                                0.0          \n",
      "3                                                0.0          \n",
      "4                                                0.0          \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getSimCountryIso  \\\n",
      "0                                                0.0        \n",
      "1                                                1.0        \n",
      "2                                                0.0        \n",
      "3                                                1.0        \n",
      "4                                                0.0        \n",
      "\n",
      "   Landroid/telephony/TelephonyManager;->getSimSerialNumber  \\\n",
      "0                                                0.0          \n",
      "1                                                0.0          \n",
      "2                                                0.0          \n",
      "3                                                0.0          \n",
      "4                                                0.0          \n",
      "\n",
      "   Lorg/apache/http/impl/client/DefaultHttpClient;->execute  Label  \n",
      "0                                                1.0           1.0  \n",
      "1                                                0.0           1.0  \n",
      "2                                                0.0           1.0  \n",
      "3                                                0.0           1.0  \n",
      "4                                                0.0           1.0  \n",
      "\n",
      "[5 rows x 242 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4464 entries, 0 to 4463\n",
      "Columns: 242 entries, ACCESS_ALL_DOWNLOADS to Label\n",
      "dtypes: float64(242)\n",
      "memory usage: 8.2 MB\n",
      "None\n",
      "Class Distribution:\n",
      "Label\n",
      "1.0    3565\n",
      "0.0     899\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diogo\\AppData\\Local\\Temp\\ipykernel_26700\\1405297251.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=df[\"Label\"], palette=\"coolwarm\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1.0    3565\n",
       "0.0     899\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"tuandromd_clean.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(df[\"Label\"].value_counts())\n",
    "\n",
    "sns.countplot(x=df[\"Label\"], palette=\"coolwarm\")\n",
    "plt.title(\"Class Distribution (Goodware vs Malware)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks([0, 1], [\"Goodware\", \"Malware\"])\n",
    "plt.show()\n",
    "\n",
    "df[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração do Pré-processamento e Hiperparâmetros  \n",
    "\n",
    "Para garantir que os modelos, sejam treinados de forma **estruturada e reprodutível**, foi realizada uma configuração inicial do **dataset**, da **validação cruzada** e dos **hiperparâmetros** a serem testados.  \n",
    "\n",
    "### O que foi feito  \n",
    "\n",
    "- **Separação das features (X) e do rótulo (y)** para que os modelos possam aprender apenas a partir das permissões das aplicações.  \n",
    "- **Configuração da Validação Cruzada (StratifiedKFold)** com **10 folds** (metodologia abordada no artigo), garantindo uma distribuição equilibrada das classes em cada divisão do treino.  \n",
    "- **Definição dos hiperparâmetros** que serão testados para encontrar a melhor configuração:  \n",
    "  - `n_estimators`: {100, 200, 300} → Número de árvores nos modelos baseados em ensembles.  \n",
    "  - `max_depth`: {5, 10, None} → Profundidade máxima das árvores de decisão.  \n",
    "  - `learning_rate`: {0.01, 0.1, 0.3} → Taxa de aprendizagem para os modelos de boosting.  \n",
    "- **Criação de todas as combinações possíveis** de hiperparâmetros com a utilização de `itertools.product()`.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools \n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "X = df.drop(columns=[\"Label\"])\n",
    "y = df[\"Label\"]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "n_estimators_values = [100, 200, 300]\n",
    "max_depth_values = [5, 10, None] \n",
    "learning_rate_values = [0.01, 0.1, 0.3]  \n",
    "random_state = 42\n",
    " \n",
    "hyperparameter_combinations = list(itertools.product(n_estimators_values, max_depth_values, learning_rate_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest   \n",
    "\n",
    "### O que foi feito  \n",
    "- Foram testados diferentes **números de árvores (n_estimators)** e **profundidades máximas das árvores (max_depth)**.  \n",
    "- Foi utilizada a **validação cruzada (10-Fold Cross-Validation)**, tal como foi utilizado no estudo abordado no artigo.  \n",
    "- O modelo foi treinado em **todo o conjunto de dados**, e as previsões foram avaliadas utilizando **métricas de desempenho**:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "  - **F1-Score**  \n",
    "\n",
    "### Hiperparâmetros Testados  \n",
    "- `n_estimators`: {100, 200, 300}: Número de árvores. Um valor maior pode melhorar a precisão, mas aumenta o tempo de treino.   \n",
    "- `max_depth`: {5, 10, None}: Profundidade máxima das árvores, controla a complexidade do modelo para evitar overfitting. \n",
    "- `random_state` = 42: Define uma semente fixa para garantir que os resultados sejam reproduzíveis.  \n",
    "- `class_weight=\"balanced\"`: Ajusta o peso das classes para lidar com \"desequilíbrio\" no conjunto de dados.  \n",
    "\n",
    "### Avaliação  \n",
    "Cada combinação de hiperparâmetros foi testada e os resultados foram armazenados num DataFrame para comparação. O objetivo foi identificar a configuração que melhor **replica** os resultados obtidos no estudo.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Random Forest Algorithm with n_estimators=100, max_depth=5\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 96.6%\n",
      "Precision: 99.6% | Recall: 96.1% | F1-Score: 96.1%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=100, max_depth=10\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.7% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=100, max_depth=None\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 99.3%\n",
      "Precision: 99.6% | Recall: 99.6% | F1-Score: 99.6%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=200, max_depth=5\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 97.1%\n",
      "Precision: 99.6% | Recall: 96.7% | F1-Score: 96.7%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=200, max_depth=10\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.8% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=200, max_depth=None\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 99.5%\n",
      "Precision: 99.7% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=300, max_depth=5\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 97.0%\n",
      "Precision: 99.6% | Recall: 96.7% | F1-Score: 96.7%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=300, max_depth=10\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.8% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing Random Forest Algorithm with n_estimators=300, max_depth=None\n",
      "\n",
      "Random Forest - Cross-Validation Accuracy: 99.5%\n",
      "Precision: 99.7% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.595389</td>\n",
       "      <td>99.621306</td>\n",
       "      <td>96.101879</td>\n",
       "      <td>96.101879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.678434</td>\n",
       "      <td>99.745674</td>\n",
       "      <td>98.597473</td>\n",
       "      <td>98.597473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.305484</td>\n",
       "      <td>99.580216</td>\n",
       "      <td>99.551034</td>\n",
       "      <td>99.551034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97.088061</td>\n",
       "      <td>99.596358</td>\n",
       "      <td>96.746530</td>\n",
       "      <td>96.746530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.700956</td>\n",
       "      <td>99.773678</td>\n",
       "      <td>98.597630</td>\n",
       "      <td>98.597630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.507178</td>\n",
       "      <td>99.692345</td>\n",
       "      <td>99.691483</td>\n",
       "      <td>99.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97.043268</td>\n",
       "      <td>99.596604</td>\n",
       "      <td>96.690350</td>\n",
       "      <td>96.690350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.678585</td>\n",
       "      <td>99.773678</td>\n",
       "      <td>98.569619</td>\n",
       "      <td>98.569619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.507178</td>\n",
       "      <td>99.692345</td>\n",
       "      <td>99.691483</td>\n",
       "      <td>99.691483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  CV Accuracy  Precision     Recall   F1-Score\n",
       "0           100        5.0    96.595389  99.621306  96.101879  96.101879\n",
       "1           100       10.0    98.678434  99.745674  98.597473  98.597473\n",
       "2           100        NaN    99.305484  99.580216  99.551034  99.551034\n",
       "3           200        5.0    97.088061  99.596358  96.746530  96.746530\n",
       "4           200       10.0    98.700956  99.773678  98.597630  98.597630\n",
       "5           200        NaN    99.507178  99.692345  99.691483  99.691483\n",
       "6           300        5.0    97.043268  99.596604  96.690350  96.690350\n",
       "7           300       10.0    98.678585  99.773678  98.569619  98.569619\n",
       "8           300        NaN    99.507178  99.692345  99.691483  99.691483"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "rf_hyperparameter_combinations = list(itertools.product(n_estimators_values, max_depth_values))\n",
    "\n",
    "for (n_estimators, max_depth) in rf_hyperparameter_combinations:\n",
    "    print(f\"Testing Random Forest Algorithm with n_estimators={n_estimators}, max_depth={max_depth}\\n\")\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                   random_state=random_state, class_weight=\"balanced\")\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    \n",
    "    precision = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "    mean_precision = np.mean(precision)\n",
    "    \n",
    "    recall = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "    mean_recall = np.mean(recall)\n",
    "    \n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "    mean_f1 = np.mean(recall)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    rf_results.append([n_estimators, max_depth, mean_accuracy * 100, mean_precision * 100, mean_recall * 100, mean_f1 * 100])\n",
    "\n",
    "    print(f\"Random Forest - Cross-Validation Accuracy: {mean_accuracy * 100:.1f}%\")\n",
    "    print(f\"Precision: {mean_precision * 100:.1f}% | Recall: {mean_recall * 100:.1f}% | F1-Score: {mean_f1 * 100:.1f}%\\n\")\n",
    "\n",
    "\n",
    "rf_results_df = pd.DataFrame(rf_results, columns=[\"n_estimators\", \"max_depth\", \"CV Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "display(rf_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees  \n",
    "\n",
    "### O que foi feito  \n",
    "- Foram testados diferentes **números de árvores (n_estimators)** e **profundidades máximas das árvores (max_depth)**.  \n",
    "- Foi utilizada a **validação cruzada (10-Fold Cross-Validation)**, tal como no estudo abordado no artigo.  \n",
    "- O modelo foi treinado em **todo o conjunto de dados**, e as previsões foram avaliadas utilizando **métricas de desempenho**:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "  - **F1-Score**  \n",
    "\n",
    "### Hiperparâmetros Testados  \n",
    "- `n_estimators`: {100, 200, 300}: Número de árvores. Um valor maior pode melhorar a precisão, mas aumenta o tempo de treino.  \n",
    "- `max_depth`: {5, 10, None}: Profundidade máxima das árvores, controlando a complexidade do modelo para evitar overfitting.  \n",
    "- `random_state` = 42: Define uma semente fixa para garantir reprodutibilidade dos resultados.  \n",
    "- `class_weight=\"balanced\"`: Ajusta o peso das classes para lidar com \"desequilíbrio\" no conjunto de dados.  \n",
    "\n",
    "### Avaliação  \n",
    "Cada combinação de hiperparâmetros foi testada e os resultados foram armazenados num DataFrame para comparação. O objetivo foi identificar a configuração que melhor **replica** os resultados obtidos no estudo.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Extra Trees Algorithm with n_estimators=100, max_depth=5\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 97.4%\n",
      "Precision: 99.6% | Recall: 97.1% | F1-Score: 97.1%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=100, max_depth=10\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.9% | Recall: 98.7% | F1-Score: 98.7%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=100, max_depth=None\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 99.5%\n",
      "Precision: 99.8% | Recall: 99.6% | F1-Score: 99.6%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=200, max_depth=5\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 97.3%\n",
      "Precision: 99.7% | Recall: 96.9% | F1-Score: 96.9%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=200, max_depth=10\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 98.8%\n",
      "Precision: 99.9% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=200, max_depth=None\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 99.5%\n",
      "Precision: 99.8% | Recall: 99.6% | F1-Score: 99.6%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=300, max_depth=5\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 97.2%\n",
      "Precision: 99.7% | Recall: 96.8% | F1-Score: 96.8%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=300, max_depth=10\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.9% | Recall: 98.7% | F1-Score: 98.7%\n",
      "\n",
      "Testing Extra Trees Algorithm with n_estimators=300, max_depth=None\n",
      "\n",
      "Extra Trees - Cross-Validation Accuracy: 99.5%\n",
      "Precision: 99.8% | Recall: 99.6% | F1-Score: 99.6%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97.356919</td>\n",
       "      <td>99.626504</td>\n",
       "      <td>97.055204</td>\n",
       "      <td>97.055204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.857606</td>\n",
       "      <td>99.886763</td>\n",
       "      <td>98.681585</td>\n",
       "      <td>98.681585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.462235</td>\n",
       "      <td>99.775433</td>\n",
       "      <td>99.551034</td>\n",
       "      <td>99.551034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97.267283</td>\n",
       "      <td>99.654500</td>\n",
       "      <td>96.914912</td>\n",
       "      <td>96.914912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.768120</td>\n",
       "      <td>99.886519</td>\n",
       "      <td>98.569540</td>\n",
       "      <td>98.569540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.462235</td>\n",
       "      <td>99.775433</td>\n",
       "      <td>99.551034</td>\n",
       "      <td>99.551034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97.200068</td>\n",
       "      <td>99.711474</td>\n",
       "      <td>96.774541</td>\n",
       "      <td>96.774541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.880077</td>\n",
       "      <td>99.886680</td>\n",
       "      <td>98.709754</td>\n",
       "      <td>98.709754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.462235</td>\n",
       "      <td>99.775433</td>\n",
       "      <td>99.551034</td>\n",
       "      <td>99.551034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  CV Accuracy  Precision     Recall   F1-Score\n",
       "0           100        5.0    97.356919  99.626504  97.055204  97.055204\n",
       "1           100       10.0    98.857606  99.886763  98.681585  98.681585\n",
       "2           100        NaN    99.462235  99.775433  99.551034  99.551034\n",
       "3           200        5.0    97.267283  99.654500  96.914912  96.914912\n",
       "4           200       10.0    98.768120  99.886519  98.569540  98.569540\n",
       "5           200        NaN    99.462235  99.775433  99.551034  99.551034\n",
       "6           300        5.0    97.200068  99.711474  96.774541  96.774541\n",
       "7           300       10.0    98.880077  99.886680  98.709754  98.709754\n",
       "8           300        NaN    99.462235  99.775433  99.551034  99.551034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_results = []\n",
    "\n",
    "et_hyperparameter_combinations = list(itertools.product(n_estimators_values, max_depth_values))\n",
    "\n",
    "for (n_estimators, max_depth) in et_hyperparameter_combinations:\n",
    "    print(f\"Testing Extra Trees Algorithm with n_estimators={n_estimators}, max_depth={max_depth}\\n\")\n",
    "    \n",
    "    model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                 random_state=random_state, class_weight=\"balanced\")\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    \n",
    "    precision = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "    mean_precision = np.mean(precision)\n",
    "    \n",
    "    recall = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "    mean_recall = np.mean(recall)\n",
    "    \n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "    mean_f1 = np.mean(recall)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    et_results.append([n_estimators, max_depth, mean_accuracy * 100, mean_precision * 100, mean_recall * 100, mean_f1 * 100])\n",
    "\n",
    "    print(f\"Extra Trees - Cross-Validation Accuracy: {mean_accuracy * 100:.1f}%\")\n",
    "    print(f\"Precision: {mean_precision * 100:.1f}% | Recall: {mean_recall * 100:.1f}% | F1-Score: {mean_f1 * 100:.1f}%\\n\")\n",
    "\n",
    "et_results_df = pd.DataFrame(et_results, columns=[\"n_estimators\", \"max_depth\", \"CV Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "display(et_results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost  \n",
    "\n",
    "### O que foi feito  \n",
    "- Foram testados diferentes **números de árvores (n_estimators)**, **profundidades máximas das árvores (max_depth)** e **taxas de aprendizagem (learning_rate)**.  \n",
    "- Foi utilizada a **validação cruzada (10-Fold Cross-Validation)** para garantir a robustez dos resultados.  \n",
    "- O modelo foi treinado em **todo o conjunto de dados**, e as previsões foram avaliadas utilizando **métricas de desempenho**:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "  - **F1-Score**  \n",
    "\n",
    "### Hiperparâmetros Testados  \n",
    "- `n_estimators`: {100, 200, 300}: Número de árvores. Um valor maior pode melhorar a precisão, mas aumenta o tempo de treino.  \n",
    "- `max_depth`: {5, 10, None}: Profundidade máxima das árvores, controlando a complexidade do modelo para evitar overfitting.  \n",
    "- `learning_rate`: {0.01, 0.1, 0.3}: Taxa de aprendizagem que controla a contribuição de cada árvore no modelo final.  \n",
    "- `random_state` = 42: Define uma semente fixa para garantir reprodutibilidade dos resultados.  \n",
    "- `eval_metric=\"logloss\"`: Utilizado para calcular a função de perda durante o treino do modelo.  \n",
    "- `scale_pos_weight`: Ajusta o peso das classes para lidar com o \"desequilíbrio\" no conjunto de dados.  \n",
    "\n",
    "### Avaliação  \n",
    "Cada combinação de hiperparâmetros foi testada e os resultados foram armazenados num DataFrame para comparação. O objetivo foi identificar a configuração que melhor **replica** os resultados obtidos no estudo.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=5, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.4%\n",
      "Precision: 99.5% | Recall: 97.2% | F1-Score: 97.2%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=5, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.6%\n",
      "Precision: 99.6% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=5, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.8%\n",
      "Precision: 99.6% | Recall: 98.9% | F1-Score: 98.9%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=10, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.3%\n",
      "Precision: 99.5% | Recall: 97.1% | F1-Score: 97.1%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=10, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.6%\n",
      "Precision: 99.6% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=10, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.5% | Recall: 99.0% | F1-Score: 99.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=None, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.3%\n",
      "Precision: 99.5% | Recall: 97.2% | F1-Score: 97.2%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=None, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.5%\n",
      "Precision: 99.6% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=100, max_depth=None, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.6% | Recall: 99.0% | F1-Score: 99.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=5, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.6%\n",
      "Precision: 99.5% | Recall: 97.5% | F1-Score: 97.5%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=5, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.7% | Recall: 98.8% | F1-Score: 98.8%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=5, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 99.0%\n",
      "Precision: 99.6% | Recall: 99.1% | F1-Score: 99.1%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=10, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.7%\n",
      "Precision: 99.5% | Recall: 97.5% | F1-Score: 97.5%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=10, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.8%\n",
      "Precision: 99.5% | Recall: 99.0% | F1-Score: 99.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=10, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.5% | Recall: 99.1% | F1-Score: 99.1%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=None, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.6%\n",
      "Precision: 99.5% | Recall: 97.4% | F1-Score: 97.4%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=None, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.6% | Recall: 98.8% | F1-Score: 98.8%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=200, max_depth=None, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.6% | Recall: 99.1% | F1-Score: 99.1%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=5, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.8%\n",
      "Precision: 99.5% | Recall: 97.7% | F1-Score: 97.7%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=5, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.8%\n",
      "Precision: 99.6% | Recall: 98.9% | F1-Score: 98.9%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=5, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 99.1%\n",
      "Precision: 99.6% | Recall: 99.2% | F1-Score: 99.2%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=10, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.1%\n",
      "Precision: 99.6% | Recall: 98.0% | F1-Score: 98.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=10, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.6% | Recall: 99.0% | F1-Score: 99.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=10, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 99.0%\n",
      "Precision: 99.6% | Recall: 99.2% | F1-Score: 99.2%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=None, learning_rate=0.01\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 97.9%\n",
      "Precision: 99.6% | Recall: 97.8% | F1-Score: 97.8%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=None, learning_rate=0.1\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 98.9%\n",
      "Precision: 99.6% | Recall: 99.0% | F1-Score: 99.0%\n",
      "\n",
      "Testing XGBoost Algorithm with n_estimators=300, max_depth=None, learning_rate=0.3\n",
      "\n",
      "XGBoost - Cross-Validation Accuracy: 99.0%\n",
      "Precision: 99.6% | Recall: 99.2% | F1-Score: 99.2%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.379390</td>\n",
       "      <td>99.513676</td>\n",
       "      <td>97.195260</td>\n",
       "      <td>97.195260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.566427</td>\n",
       "      <td>99.633236</td>\n",
       "      <td>98.569619</td>\n",
       "      <td>98.569619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.835285</td>\n",
       "      <td>99.606971</td>\n",
       "      <td>98.934315</td>\n",
       "      <td>98.934315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.312126</td>\n",
       "      <td>99.484863</td>\n",
       "      <td>97.139159</td>\n",
       "      <td>97.139159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.588798</td>\n",
       "      <td>99.605933</td>\n",
       "      <td>98.625720</td>\n",
       "      <td>98.625720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.857706</td>\n",
       "      <td>99.523487</td>\n",
       "      <td>99.046596</td>\n",
       "      <td>99.046596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.334547</td>\n",
       "      <td>99.485103</td>\n",
       "      <td>97.167170</td>\n",
       "      <td>97.167170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.544005</td>\n",
       "      <td>99.578080</td>\n",
       "      <td>98.597709</td>\n",
       "      <td>98.597709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.902499</td>\n",
       "      <td>99.578723</td>\n",
       "      <td>99.046596</td>\n",
       "      <td>99.046596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.648148</td>\n",
       "      <td>99.515316</td>\n",
       "      <td>97.531788</td>\n",
       "      <td>97.531788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.745649</td>\n",
       "      <td>99.662361</td>\n",
       "      <td>98.766012</td>\n",
       "      <td>98.766012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.969764</td>\n",
       "      <td>99.634903</td>\n",
       "      <td>99.074607</td>\n",
       "      <td>99.074607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.670519</td>\n",
       "      <td>99.543479</td>\n",
       "      <td>97.531631</td>\n",
       "      <td>97.531631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.812813</td>\n",
       "      <td>99.523169</td>\n",
       "      <td>98.990416</td>\n",
       "      <td>98.990416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.902549</td>\n",
       "      <td>99.523953</td>\n",
       "      <td>99.102619</td>\n",
       "      <td>99.102619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.558562</td>\n",
       "      <td>99.542661</td>\n",
       "      <td>97.391417</td>\n",
       "      <td>97.391417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.655963</td>\n",
       "      <td>99.550859</td>\n",
       "      <td>98.765855</td>\n",
       "      <td>98.765855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.924971</td>\n",
       "      <td>99.551576</td>\n",
       "      <td>99.102619</td>\n",
       "      <td>99.102619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.782677</td>\n",
       "      <td>99.515725</td>\n",
       "      <td>97.700091</td>\n",
       "      <td>97.700091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.835234</td>\n",
       "      <td>99.635137</td>\n",
       "      <td>98.906147</td>\n",
       "      <td>98.906147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>300</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>99.059249</td>\n",
       "      <td>99.635060</td>\n",
       "      <td>99.186652</td>\n",
       "      <td>99.186652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>300</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>98.140970</td>\n",
       "      <td>99.630594</td>\n",
       "      <td>98.036777</td>\n",
       "      <td>98.036777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>300</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.924870</td>\n",
       "      <td>99.606973</td>\n",
       "      <td>99.046596</td>\n",
       "      <td>99.046596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>300</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>98.969663</td>\n",
       "      <td>99.551729</td>\n",
       "      <td>99.158641</td>\n",
       "      <td>99.158641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.916855</td>\n",
       "      <td>99.573120</td>\n",
       "      <td>97.812136</td>\n",
       "      <td>97.812136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.857656</td>\n",
       "      <td>99.578959</td>\n",
       "      <td>98.990495</td>\n",
       "      <td>98.990495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.30</td>\n",
       "      <td>99.014406</td>\n",
       "      <td>99.579662</td>\n",
       "      <td>99.186652</td>\n",
       "      <td>99.186652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  learning_rate  CV Accuracy  Precision     Recall  \\\n",
       "0            100        5.0           0.01    97.379390  99.513676  97.195260   \n",
       "1            100        5.0           0.10    98.566427  99.633236  98.569619   \n",
       "2            100        5.0           0.30    98.835285  99.606971  98.934315   \n",
       "3            100       10.0           0.01    97.312126  99.484863  97.139159   \n",
       "4            100       10.0           0.10    98.588798  99.605933  98.625720   \n",
       "5            100       10.0           0.30    98.857706  99.523487  99.046596   \n",
       "6            100        NaN           0.01    97.334547  99.485103  97.167170   \n",
       "7            100        NaN           0.10    98.544005  99.578080  98.597709   \n",
       "8            100        NaN           0.30    98.902499  99.578723  99.046596   \n",
       "9            200        5.0           0.01    97.648148  99.515316  97.531788   \n",
       "10           200        5.0           0.10    98.745649  99.662361  98.766012   \n",
       "11           200        5.0           0.30    98.969764  99.634903  99.074607   \n",
       "12           200       10.0           0.01    97.670519  99.543479  97.531631   \n",
       "13           200       10.0           0.10    98.812813  99.523169  98.990416   \n",
       "14           200       10.0           0.30    98.902549  99.523953  99.102619   \n",
       "15           200        NaN           0.01    97.558562  99.542661  97.391417   \n",
       "16           200        NaN           0.10    98.655963  99.550859  98.765855   \n",
       "17           200        NaN           0.30    98.924971  99.551576  99.102619   \n",
       "18           300        5.0           0.01    97.782677  99.515725  97.700091   \n",
       "19           300        5.0           0.10    98.835234  99.635137  98.906147   \n",
       "20           300        5.0           0.30    99.059249  99.635060  99.186652   \n",
       "21           300       10.0           0.01    98.140970  99.630594  98.036777   \n",
       "22           300       10.0           0.10    98.924870  99.606973  99.046596   \n",
       "23           300       10.0           0.30    98.969663  99.551729  99.158641   \n",
       "24           300        NaN           0.01    97.916855  99.573120  97.812136   \n",
       "25           300        NaN           0.10    98.857656  99.578959  98.990495   \n",
       "26           300        NaN           0.30    99.014406  99.579662  99.186652   \n",
       "\n",
       "     F1-Score  \n",
       "0   97.195260  \n",
       "1   98.569619  \n",
       "2   98.934315  \n",
       "3   97.139159  \n",
       "4   98.625720  \n",
       "5   99.046596  \n",
       "6   97.167170  \n",
       "7   98.597709  \n",
       "8   99.046596  \n",
       "9   97.531788  \n",
       "10  98.766012  \n",
       "11  99.074607  \n",
       "12  97.531631  \n",
       "13  98.990416  \n",
       "14  99.102619  \n",
       "15  97.391417  \n",
       "16  98.765855  \n",
       "17  99.102619  \n",
       "18  97.700091  \n",
       "19  98.906147  \n",
       "20  99.186652  \n",
       "21  98.036777  \n",
       "22  99.046596  \n",
       "23  99.158641  \n",
       "24  97.812136  \n",
       "25  98.990495  \n",
       "26  99.186652  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "xgb_hyperparameter_combinations = list(itertools.product(n_estimators_values, max_depth_values, learning_rate_values))\n",
    "\n",
    "for (n_estimators, max_depth, learning_rate) in xgb_hyperparameter_combinations:\n",
    "    print(f\"Testing XGBoost Algorithm with n_estimators={n_estimators}, max_depth={max_depth}, learning_rate={learning_rate}\\n\")\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          random_state=random_state, eval_metric=\"logloss\",\n",
    "                          scale_pos_weight=(y.value_counts()[0] / y.value_counts()[1]))\n",
    "\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(scores)\n",
    "    \n",
    "    precision = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "    mean_precision = np.mean(precision)\n",
    "    \n",
    "    recall = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "    mean_recall = np.mean(recall)\n",
    "    \n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "    mean_f1 = np.mean(recall)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    xgb_results.append([n_estimators, max_depth, learning_rate, mean_accuracy * 100, mean_precision * 100, mean_recall * 100, mean_f1 * 100])\n",
    "\n",
    "    print(f\"XGBoost - Cross-Validation Accuracy: {mean_accuracy * 100:.1f}%\")\n",
    "    print(f\"Precision: {mean_precision * 100:.1f}% | Recall: {mean_recall * 100:.1f}% | F1-Score: {mean_f1 * 100:.1f}%\\n\")\n",
    "\n",
    "xgb_results_df = pd.DataFrame(xgb_results, columns=[\"n_estimators\", \"max_depth\", \"learning_rate\", \"CV Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "display(xgb_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost & Gradient Boosting  \n",
    "\n",
    "### O que foi feito  \n",
    "- Foram testados diferentes **valores de n_estimators** (número de modelos fracos no ensemble) e **learning_rate** (peso de cada novo modelo).  \n",
    "- Foi utilizada a **validação cruzada (10-Fold Cross-Validation)** para avaliar a performance dos modelos de forma consistente.  \n",
    "- O modelo foi treinado em **todo o conjunto de dados**, e avaliámos os resultados utilizando **métricas de desempenho**:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "  - **F1-Score**  \n",
    "\n",
    "### Hiperparâmetros Testados  \n",
    "- `n_estimators`: {100, 200, 300}: Número de árvores. Um valor maior pode melhorar a precisão, mas aumenta o tempo de treino  \n",
    "- `learning_rate`: {0.01, 0.1, 0.3}: Taxa de aprendizagem que controla a contribuição de cada árvore no modelo final. \n",
    "- `random_state` = 42: Define uma semente fixa para garantir reprodutibilidade dos resultados.  \n",
    "\n",
    "### Avaliação  \n",
    "Cada combinação de hiperparâmetros foi testada e os resultados foram armazenados num DataFrame para comparação. O objetivo foi identificar a configuração que melhor **replica** os resultados obtidos no estudo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AdaBoost Algorithm with n_estimators=100, learning_rate=0.01\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 92.8%\n",
      "Precision: 93.4% | Recall: 98.0% | F1-Score: 98.0%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=100, learning_rate=0.1\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 96.6%\n",
      "Precision: 97.9% | Recall: 97.8% | F1-Score: 97.8%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=100, learning_rate=0.3\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 97.1%\n",
      "Precision: 98.2% | Recall: 98.1% | F1-Score: 98.1%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=200, learning_rate=0.01\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 95.4%\n",
      "Precision: 96.6% | Recall: 97.7% | F1-Score: 97.7%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=200, learning_rate=0.1\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 96.8%\n",
      "Precision: 98.0% | Recall: 98.0% | F1-Score: 98.0%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=200, learning_rate=0.3\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 97.8%\n",
      "Precision: 98.6% | Recall: 98.7% | F1-Score: 98.7%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=300, learning_rate=0.01\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 95.4%\n",
      "Precision: 96.6% | Recall: 97.7% | F1-Score: 97.7%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=300, learning_rate=0.1\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 97.0%\n",
      "Precision: 98.2% | Recall: 98.1% | F1-Score: 98.1%\n",
      "\n",
      "Testing AdaBoost Algorithm with n_estimators=300, learning_rate=0.3\n",
      "\n",
      "AdaBoost - Cross-Validation Accuracy: 97.9%\n",
      "Precision: 98.7% | Recall: 98.7% | F1-Score: 98.7%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=100, learning_rate=0.01\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 97.0%\n",
      "Precision: 97.0% | Recall: 99.4% | F1-Score: 99.4%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=100, learning_rate=0.1\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 98.7%\n",
      "Precision: 99.0% | Recall: 99.4% | F1-Score: 99.4%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=100, learning_rate=0.3\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 99.3%\n",
      "Precision: 99.5% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=200, learning_rate=0.01\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 97.1%\n",
      "Precision: 97.8% | Recall: 98.6% | F1-Score: 98.6%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=200, learning_rate=0.1\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 99.1%\n",
      "Precision: 99.3% | Recall: 99.6% | F1-Score: 99.6%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=200, learning_rate=0.3\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 99.4%\n",
      "Precision: 99.6% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=300, learning_rate=0.01\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 97.8%\n",
      "Precision: 98.6% | Recall: 98.7% | F1-Score: 98.7%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=300, learning_rate=0.1\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 99.2%\n",
      "Precision: 99.4% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n",
      "Testing Gradient Boosting Algorithm with n_estimators=300, learning_rate=0.3\n",
      "\n",
      "Gradient Boosting - Cross-Validation Accuracy: 99.4%\n",
      "Precision: 99.6% | Recall: 99.7% | F1-Score: 99.7%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>92.809763</td>\n",
       "      <td>93.356254</td>\n",
       "      <td>98.008372</td>\n",
       "      <td>98.008372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>96.550195</td>\n",
       "      <td>97.925584</td>\n",
       "      <td>97.756192</td>\n",
       "      <td>97.756192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>100</td>\n",
       "      <td>0.30</td>\n",
       "      <td>97.065489</td>\n",
       "      <td>98.208339</td>\n",
       "      <td>98.120810</td>\n",
       "      <td>98.120810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>95.430222</td>\n",
       "      <td>96.628057</td>\n",
       "      <td>97.700170</td>\n",
       "      <td>97.700170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>96.819103</td>\n",
       "      <td>98.012257</td>\n",
       "      <td>98.008608</td>\n",
       "      <td>98.008608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>200</td>\n",
       "      <td>0.30</td>\n",
       "      <td>97.804898</td>\n",
       "      <td>98.601627</td>\n",
       "      <td>98.653810</td>\n",
       "      <td>98.653810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>95.430222</td>\n",
       "      <td>96.628057</td>\n",
       "      <td>97.700170</td>\n",
       "      <td>97.700170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>97.043168</td>\n",
       "      <td>98.180242</td>\n",
       "      <td>98.120810</td>\n",
       "      <td>98.120810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>300</td>\n",
       "      <td>0.30</td>\n",
       "      <td>97.872112</td>\n",
       "      <td>98.685181</td>\n",
       "      <td>98.653810</td>\n",
       "      <td>98.653810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.043218</td>\n",
       "      <td>96.968840</td>\n",
       "      <td>99.411135</td>\n",
       "      <td>99.411135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>98.723327</td>\n",
       "      <td>98.997718</td>\n",
       "      <td>99.411135</td>\n",
       "      <td>99.411135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>100</td>\n",
       "      <td>0.30</td>\n",
       "      <td>99.328057</td>\n",
       "      <td>99.468877</td>\n",
       "      <td>99.691483</td>\n",
       "      <td>99.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.088161</td>\n",
       "      <td>97.753913</td>\n",
       "      <td>98.625799</td>\n",
       "      <td>98.625799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>99.104092</td>\n",
       "      <td>99.302053</td>\n",
       "      <td>99.579360</td>\n",
       "      <td>99.579360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>200</td>\n",
       "      <td>0.30</td>\n",
       "      <td>99.417642</td>\n",
       "      <td>99.579829</td>\n",
       "      <td>99.691483</td>\n",
       "      <td>99.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>97.804797</td>\n",
       "      <td>98.577204</td>\n",
       "      <td>98.681742</td>\n",
       "      <td>98.681742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>99.238471</td>\n",
       "      <td>99.357612</td>\n",
       "      <td>99.691483</td>\n",
       "      <td>99.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>300</td>\n",
       "      <td>0.30</td>\n",
       "      <td>99.440064</td>\n",
       "      <td>99.579987</td>\n",
       "      <td>99.719573</td>\n",
       "      <td>99.719573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Algorithm  n_estimators  learning_rate  CV Accuracy  Precision  \\\n",
       "0            AdaBoost           100           0.01    92.809763  93.356254   \n",
       "1            AdaBoost           100           0.10    96.550195  97.925584   \n",
       "2            AdaBoost           100           0.30    97.065489  98.208339   \n",
       "3            AdaBoost           200           0.01    95.430222  96.628057   \n",
       "4            AdaBoost           200           0.10    96.819103  98.012257   \n",
       "5            AdaBoost           200           0.30    97.804898  98.601627   \n",
       "6            AdaBoost           300           0.01    95.430222  96.628057   \n",
       "7            AdaBoost           300           0.10    97.043168  98.180242   \n",
       "8            AdaBoost           300           0.30    97.872112  98.685181   \n",
       "9   Gradient Boosting           100           0.01    97.043218  96.968840   \n",
       "10  Gradient Boosting           100           0.10    98.723327  98.997718   \n",
       "11  Gradient Boosting           100           0.30    99.328057  99.468877   \n",
       "12  Gradient Boosting           200           0.01    97.088161  97.753913   \n",
       "13  Gradient Boosting           200           0.10    99.104092  99.302053   \n",
       "14  Gradient Boosting           200           0.30    99.417642  99.579829   \n",
       "15  Gradient Boosting           300           0.01    97.804797  98.577204   \n",
       "16  Gradient Boosting           300           0.10    99.238471  99.357612   \n",
       "17  Gradient Boosting           300           0.30    99.440064  99.579987   \n",
       "\n",
       "       Recall   F1-Score  \n",
       "0   98.008372  98.008372  \n",
       "1   97.756192  97.756192  \n",
       "2   98.120810  98.120810  \n",
       "3   97.700170  97.700170  \n",
       "4   98.008608  98.008608  \n",
       "5   98.653810  98.653810  \n",
       "6   97.700170  97.700170  \n",
       "7   98.120810  98.120810  \n",
       "8   98.653810  98.653810  \n",
       "9   99.411135  99.411135  \n",
       "10  99.411135  99.411135  \n",
       "11  99.691483  99.691483  \n",
       "12  98.625799  98.625799  \n",
       "13  99.579360  99.579360  \n",
       "14  99.691483  99.691483  \n",
       "15  98.681742  98.681742  \n",
       "16  99.691483  99.691483  \n",
       "17  99.719573  99.719573  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "boosting_results = []\n",
    "\n",
    "boosting_hyperparameter_combinations = list(itertools.product(n_estimators_values, learning_rate_values))\n",
    "\n",
    "for model_name, ModelClass in {\"AdaBoost\": AdaBoostClassifier, \"Gradient Boosting\": GradientBoostingClassifier}.items():\n",
    "    for (n_estimators, learning_rate) in boosting_hyperparameter_combinations:\n",
    "        print(f\"Testing {model_name} Algorithm with n_estimators={n_estimators}, learning_rate={learning_rate}\\n\")\n",
    "        \n",
    "        model = ModelClass(n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)\n",
    "\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "        mean_accuracy = np.mean(scores)\n",
    "        \n",
    "        precision = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "        mean_precision = np.mean(precision)\n",
    "        \n",
    "        recall = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "        mean_recall = np.mean(recall)\n",
    "        \n",
    "        f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "        mean_f1 = np.mean(recall)\n",
    "\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        boosting_results.append([model_name, n_estimators, learning_rate, mean_accuracy * 100, mean_precision * 100, mean_recall * 100, mean_f1 * 100])\n",
    "\n",
    "        print(f\"{model_name} - Cross-Validation Accuracy: {mean_accuracy * 100:.1f}%\")\n",
    "        print(f\"Precision: {mean_precision * 100:.1f}% | Recall: {mean_recall * 100:.1f}% | F1-Score: {mean_f1 * 100:.1f}%\\n\")\n",
    "\n",
    "boosting_results_df = pd.DataFrame(boosting_results, columns=[\"Algorithm\", \"n_estimators\", \"learning_rate\", \"CV Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "display(boosting_results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão  \n",
    "\n",
    "Neste estudo, foram testados diferentes modelos de **Machine Learning** para a deteção de **malware em aplicações Android**, sendo aplicada a abordagem utilizada no artigo original.  \n",
    "\n",
    "Foram avaliadas várias combinações de **hiperparâmetros** através de **validação cruzada (10-Fold Cross-Validation)**, e os resultados foram comparados com os valores apresentados no artigo.  \n",
    "\n",
    "Abaixo, é apresentada a melhor configuração encontrada para cada algoritmo, ou seja, aquela cuja **\"Accuracy\" mais se aproxima** dos valores do estudo original.  \n",
    "\n",
    "### Melhor Configuração por Algoritmo  \n",
    "\n",
    "| Algoritmo            | Melhor n_estimators | Melhor max_depth | Melhor learning_rate | CV Accuracy Apresentado | CV Accuracy (Artigo) | Precision | Recall | F1-Score |\n",
    "|----------------------|--------------------|------------------|----------------------|---------------------------|-----------------------|------------|--------|---------|\n",
    "| **Random Forest**    | 100                | 10               | -                    | **98.7%**                 | 98.7%                 | **99.7%**  | **98.6%** | **98.6%** |\n",
    "| **Extra Trees**      | 100                | 10               | -                    | **98.9%**                 | 98.8%                 | **99.9%**  | **98.7%** | **98.7%** |\n",
    "| **AdaBoost**         | 300                | -                | 0.3                  | **97.9%**                 | 97.9%                 | **98.7%**  | **98.7%** | **98.7%** |\n",
    "| **XGBoost**          | 100                | 5                | 0.1                  | **97.8%**                 | 97.8%                 | **99.6%**  | **98.6%** | **98.6%** |\n",
    "| **Gradient Boosting** | 100                | -                | 0.01                 | **97.4%**                 | 97.4%                 | **97.0%**  | **99.4%** | **99.4%** |\n",
    "\n",
    "\n",
    "### Conclusões Finais  \n",
    "- Foi conseguido **replicar com sucesso** os resultados apresentados no estudo original.  \n",
    "- Pequenas variações podem ocorrer devido à forma como os dados são processados e às diferenças na implementação dos algoritmos, uma vez que falta alguma informação na metodologia adotada no artigo.  \n",
    "- O **Random Forest e o Extra Trees** foram os modelos mais eficazes, atingindo valores superiores a **98% de \"Accuracy\"**, sendo que os modelos de Boosting (**AdaBoost, XGBoost e Gradient Boosting**) também apresentaram valores bastante positivos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malware-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
